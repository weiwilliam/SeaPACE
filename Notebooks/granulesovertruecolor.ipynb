{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4070b-1490-4011-8c0c-c88d3a0cce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "import earthaccess\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import functions as fc\n",
    "\n",
    "import holoviews as hv\n",
    "from bokeh.models import HoverTool\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542711b-06f3-4095-8896-4045742805fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tspan = (\"2024-09-22\", \"2024-09-28\")\n",
    "bbox = (-125., 32., -116., 38.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9798bc-ec68-4f41-aad1-173dcd398f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name=\"PACE_OCI_L2_AOP\",\n",
    "    temporal=tspan,\n",
    "    bounding_box=bbox,\n",
    "    # cloud_cover=clouds,\n",
    ")\n",
    "paths = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876b293-515f-4158-939e-739ed2fcfba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "granules_data = []\n",
    "for grmeta in tqdm(results, desc='Processing Granules'):\n",
    "    gr_name = grmeta['umm']['GranuleUR']\n",
    "    gr_time = grmeta['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime']\n",
    "    polygons = grmeta['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['GPolygons']\n",
    "    polygon_coord = [(pt['Longitude'], pt['Latitude']) for pt in polygons[0]['Boundary']['Points']]\n",
    "    granules_data.append({'time': gr_time, 'granule': gr_name, 'geometry': polygon_coord})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599acd5-6d40-43a1-b516-337ae874959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OCI_PACE_truecolor(time, size=(400, 800), bbox=(-180, -90, 180, 90)):\n",
    "    import numpy as np\n",
    "    from owslib.wms import WebMapService\n",
    "    import lxml.etree as xmltree\n",
    "    import xml.etree.ElementTree as xmlet\n",
    "    import requests\n",
    "    from skimage import io\n",
    "    \"\"\"\n",
    "      time: in format of YYYY-MM-DD\n",
    "      size: (height, width)\n",
    "      bbox: bounding box (minlon, minlat, maxlon, maxlat)\n",
    "    \"\"\"\n",
    "    height, width = size\n",
    "    minlon, minlat, maxlon, maxlat = bbox\n",
    "    #  Construct Geographic projection URL.\n",
    "    gibs_url = 'https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi?version=1.3.0&service=WMS&request=GetMap&format=image/png&STYLE=default'\n",
    "    proj4326 = f'{gibs_url}&bbox={int(minlat)},{int(minlon)},{int(maxlat)},{int(maxlon)}&CRS=EPSG:4326&HEIGHT={height}&WIDTH={width}&TIME={time}&layers=OCI_PACE_True_Color'\n",
    "    \n",
    "    # Request image.\n",
    "    img = io.imread(proj4326)\n",
    "    x = np.linspace(minlon, maxlon, img.shape[1])\n",
    "    y = np.linspace(minlat, maxlat, img.shape[0])\n",
    "    img = img[::-1, :]\n",
    "\n",
    "    return x, y, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da1f06-a580-4c8b-a417-69b56609913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) DEFINE SB_PARSER ───────────────────────────────────────────────────────\n",
    "\n",
    "def parse_sb(file_path):\n",
    "    text = Path(file_path).read_text()\n",
    "    header, body = text.split('/end_header', 1)\n",
    "\n",
    "    # 1a) extract header entries\n",
    "    metadata = {}\n",
    "    for line in header.splitlines():\n",
    "        if not line.startswith('/'): \n",
    "            continue\n",
    "        if '=' not in line:    \n",
    "            continue\n",
    "        key, val = line[1:].split('=', 1)\n",
    "        key, val = key.strip(), re.sub(r'\\[.*?\\]', '', val).strip()\n",
    "        if key in {\n",
    "            'station','data_file_name',\n",
    "            'start_date','end_date',\n",
    "            'start_time','end_time',\n",
    "            'fields',\n",
    "            'north_latitude','south_latitude',\n",
    "            'west_longitude','east_longitude'\n",
    "        }:\n",
    "            metadata[key] = val\n",
    "\n",
    "    # 1b) parse the /fields list & store it\n",
    "    fields_list = [f.strip() for f in metadata['fields'].split(',')]\n",
    "    metadata['fields_list'] = fields_list\n",
    "\n",
    "    # 1c) read the body into a DataFrame\n",
    "    df = pd.read_csv(\n",
    "        StringIO(body.strip()),\n",
    "        sep=',',\n",
    "        names=fields_list,\n",
    "        comment='/'\n",
    "    )\n",
    "\n",
    "    # 1d) detect single‐spectrum files & fill lat/lon if missing\n",
    "    is_spectrum = ('lat' not in df.columns or 'lon' not in df.columns)\n",
    "    metadata['single_spectrum'] = is_spectrum\n",
    "    if is_spectrum:\n",
    "        lat0 = float(metadata.get('north_latitude', metadata.get('south_latitude', 0)))\n",
    "        lon0 = float(metadata.get('west_longitude',  metadata.get('east_longitude',  0)))\n",
    "        df['lat'], df['lon'] = lat0, lon0\n",
    "\n",
    "    # 1e) ensure date/time exist\n",
    "    if 'date' not in df.columns:\n",
    "        df['date'] = metadata['start_date']\n",
    "    if 'time' not in df.columns:\n",
    "        df['time'] = metadata['start_time']\n",
    "\n",
    "    # 1f) build datetime column\n",
    "    df['datetime'] = pd.to_datetime(df['date'].astype(str) + ' ' + df['time'])\n",
    "\n",
    "    # 1g) parse header start/end datetimes\n",
    "    def _pd(d, t):\n",
    "        t_clean = re.sub(r'\\[.*?\\]', '', t)\n",
    "        dfmt = '%Y-%m-%d' if '-' in d else '%Y%m%d'\n",
    "        return datetime.strptime(f\"{d} {t_clean}\", f\"{dfmt} %H:%M:%S\")\n",
    "    metadata['start_datetime'] = _pd(metadata['start_date'], metadata['start_time'])\n",
    "    metadata['end_datetime']   = _pd(metadata['end_date'],   metadata['end_time'])\n",
    "\n",
    "    # 1h) keep only the five columns you care about\n",
    "    df = df[['date','time','lat','lon' '','datetime']]\n",
    "\n",
    "    return metadata, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7ec1b-89ea-4cff-ba1a-44722586ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = fc.get_dates(tspan[0], tspan[1], 24)\n",
    "imgs = {}\n",
    "for date in tqdm(dates, desc=\"Fetching true color from NASA WorldView\"):\n",
    "    daystr = date.strftime('%Y-%m-%d')\n",
    "    x, y, img = get_OCI_PACE_truecolor(daystr, size=(400, 400), bbox=bbox)\n",
    "    imgs[daystr] = hv.RGB((x, y, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1676aa-0d5b-4261-ad60-2d549fa569b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2.a) COLLECT & PREPARE ALL FILES ──────────────────────────────────────────\n",
    "\n",
    "root = Path(\"/home/jovyan/shared-public/pace-hackweek/SeePACE/Hackweek_PACE-PAX_09_22-28\")\n",
    "metadata_list = []\n",
    "search_types = ['Rrs', 'AOP'] #  Add additional search parameters if desired ***********************\n",
    "\n",
    "for sb in root.rglob('*.sb'):\n",
    "    meta, df = parse_sb(sb)\n",
    "\n",
    "    # collapse spectral files to exactly one row\n",
    "    if meta['single_spectrum']:\n",
    "        df = df.head(1)\n",
    "\n",
    "    meta['data'] = df\n",
    "    # detect which of your search_types appear in the original fields\n",
    "    meta['Data_Type'] = [\n",
    "        t for t in search_types\n",
    "        if any(t in f for f in meta['fields_list'])\n",
    "    ]\n",
    "    metadata_list.append(meta)\n",
    "\n",
    "summary_df = pd.DataFrame([{\n",
    "    'Station':        m['station'],\n",
    "    'Data_File_Name': m['data_file_name'],\n",
    "    'Start_Datetime': m['start_datetime'],\n",
    "    'End_Datetime':   m['end_datetime'],\n",
    "    'Data_Type':      m['Data_Type']\n",
    "} for m in metadata_list])\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fed558-a752-4e98-a3ec-d1787898537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 4) BUILD & CONCATENATE XR DATASETS ─────────────────────────────────────\n",
    "\n",
    "ds_list    = []\n",
    "file_names = []\n",
    "\n",
    "for m in metadata_list:\n",
    "    df = m['data'].reset_index(drop=True).reset_index().rename(columns={'index':'record'})\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            'lat':      ('record', df['lat']),\n",
    "            'lon':      ('record', df['lon']),\n",
    "            'datetime': ('record', df['datetime'])\n",
    "        },\n",
    "        coords={'record': df['record']}\n",
    "    )\n",
    "    # promote to a 2-D Dataset along new 'file' dim\n",
    "    ds = ds.expand_dims(file=[m['data_file_name']])\n",
    "    ds_list.append(ds)\n",
    "    file_names.append(m['data_file_name'])\n",
    "\n",
    "ds_combined = xr.concat(\n",
    "    ds_list,\n",
    "    dim='file',\n",
    "    coords='minimal',\n",
    "    compat='override'\n",
    ")\n",
    "\n",
    "print(ds_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7753d-70e9-447d-8477-12eeea5d7f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "granules_by_date = defaultdict(list)\n",
    "for g in granules_data:\n",
    "    date = g['time'][:10]  # 'YYYY-MM-DD'\n",
    "    granules_by_date[date].append(g)\n",
    "\n",
    "# Sort the available days\n",
    "available_days = sorted(set(imgs.keys()) | set(granules_by_date.keys()))\n",
    "\n",
    "# Date slider\n",
    "day_slider = pn.widgets.DiscreteSlider(name=\"Date\", options=available_days)\n",
    "\n",
    "# Wrap-and-merge filenames for hover\n",
    "def wrap_files(vals):\n",
    "    if len(vals)>1:\n",
    "        names = sorted(set(vals))\n",
    "        joined = \", \".join(names)\n",
    "        text = textwrap.fill(joined, width=30).replace(\"\\n\", '<br>')\n",
    "    else:\n",
    "        text = textwrap.fill(str(vals[0]), width=30).replace(\"\\n\", '<br>')\n",
    "    return text\n",
    "\n",
    "# Function to generate a polygon from selected time\n",
    "def make_granule_polygon(granules):\n",
    "    poly_data = []\n",
    "    for g in granules:\n",
    "        poly_data.append({\n",
    "            ('x', 'y'): g['geometry'],\n",
    "            'granule': wrap_files([g['granule']]),\n",
    "            'time': g['time']\n",
    "        })\n",
    "    return hv.Polygons(poly_data, vdims=['granule', 'time']).opts(\n",
    "        fill_alpha=0.3,\n",
    "        fill_color='pink',\n",
    "        line_color='red',\n",
    "        tools=[granule_hover]\n",
    "    )\n",
    "\n",
    "def make_insitu_points(df):\n",
    "    grouped = df.groupby(['lon','lat'], as_index=False).agg({\n",
    "        'datetime':       'max',\n",
    "        'File':           wrap_files,\n",
    "        'Data_Type_str':  lambda v: '<br>'.join(sorted(set(v)))\n",
    "    })\n",
    "    return hv.Points(\n",
    "        grouped,\n",
    "        kdims=['lon','lat'],\n",
    "        vdims=['datetime','File','Data_Type_str']\n",
    "    ).opts(\n",
    "        size=8, color='blue', tools=[hover, 'wheel_zoom', 'pan'], active_tools=['wheel_zoom']\n",
    "    )\n",
    "\n",
    "def make_transects_lines(df):\n",
    "    # Build transect lines per file\n",
    "    lines = []\n",
    "    for fname, sub in df.groupby('File'):\n",
    "        if len(sub) > 1:\n",
    "            # sort by datetime\n",
    "            sub_sorted = sub.sort_values('datetime')\n",
    "            coords = list(zip(sub_sorted['lon'], sub_sorted['lat']))\n",
    "            line = hv.Path([coords], kdims=['x','y']).opts(color='red', line_width=2)\n",
    "            lines.append(line)\n",
    "    # Combine all lines\n",
    "    if lines:\n",
    "        return hv.Overlay(lines)\n",
    "        # {f.name if hasattr(f, 'name') else i: f for i, f in enumerate(lines)}\n",
    "    else:\n",
    "        # Return an empty element if no line exists\n",
    "        return hv.Path([])\n",
    "\n",
    "granule_hover = HoverTool(\n",
    "    tooltips=\"\"\"\n",
    "    <div style='max-width:300px;'>\n",
    "      <strong>Granule:</strong> @granule<br>\n",
    "      <strong>Time:</strong> @time\n",
    "    </div>\n",
    "    \"\"\",\n",
    "    point_policy='follow_mouse'\n",
    ")\n",
    "\n",
    "hover = HoverTool(\n",
    "    tooltips=\"\"\"\n",
    "    <div style='max-width:200px;'>\n",
    "      <strong>Time:</strong> @datetime{%F}<br>\n",
    "      <strong>File:</strong> @File{safe}<br>\n",
    "      <strong>Type:</strong> @Data_Type_str\n",
    "    </div>\n",
    "    \"\"\",\n",
    "    formatters={'@datetime':'datetime'},\n",
    "    point_policy='snap_to_data'\n",
    ")\n",
    "\n",
    "def make_plot(selected_day=None, show_all=None):\n",
    "    base = imgs.get(selected_day)\n",
    "    if base is None:\n",
    "        return hv.Text(0, 0, f\"No image for {selected_day}\").opts(width=600, height=500)\n",
    "\n",
    "    if show_all:\n",
    "        granules = sum(granules_by_date.values(), [])\n",
    "        df = full_df.copy()\n",
    "    else:\n",
    "        granules = granules_by_date.get(selected_day, [])\n",
    "        df = full_df[full_df['datetime'].dt.date == pd.to_datetime(selected_day).date()]\n",
    "        \n",
    "    polygons = [make_granule_polygon(granules)]\n",
    "    points = [make_insitu_points(df)]\n",
    "    transects = [make_transects_lines(df)]\n",
    "\n",
    "    if not polygons:\n",
    "        return hv.Text(0, 0, \"No granules available\").opts(width=600, height=500)\n",
    "        \n",
    "    n_points = len(df)\n",
    "    elements = polygons + points + transects\n",
    "    overlay = base * hv.Overlay(elements)\n",
    "    return overlay.opts(\n",
    "        ylim=(bbox[1], bbox[3]),\n",
    "        xlim=(bbox[0], bbox[2]),\n",
    "        width=x.size,\n",
    "        height=y.size,\n",
    "        xlabel='Longitude',\n",
    "        ylabel='Latitude',\n",
    "        framewise=False,\n",
    "        title=f\"Granules on {selected_day} / {n_points} in-situ points\"\n",
    "    )\n",
    "\n",
    "toggle = pn.widgets.Checkbox(\n",
    "    name='Show All Days',\n",
    "    value=False\n",
    ")\n",
    "\n",
    "# Bind to panel\n",
    "dynamic_map = pn.bind(make_plot, selected_day=day_slider, show_all=toggle)\n",
    "\n",
    "# Layout\n",
    "app = pn.Column(\n",
    "    \"# Daily Granules and In-Situ Viewer\",\n",
    "    pn.Row(day_slider, toggle),\n",
    "    dynamic_map\n",
    ")\n",
    "\n",
    "app.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc00f66-8e38-4e87-84e9-39984daefaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
