{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bc7da-a2a0-4f42-a8bc-c3458df4c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install earthaccess\n",
    "\n",
    "#SETUP\n",
    "import earthaccess\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd590057-d4a2-4b92-bf10-4e6e04b7191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS\n",
    "tspan = (\"2024-09-22\", \"2024-09-28\")\n",
    "bbox = (-125, 32, -116, 38)\n",
    "clouds = (0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c63a13-a356-4c03-9702-45b8a0d0154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA SEARCH\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"PACE_OCI_L2_AOP\",\n",
    "    temporal=tspan,\n",
    "    bounding_box=bbox,\n",
    "    cloud_cover=clouds,\n",
    ")\n",
    "paths = earthaccess.open(results)\n",
    "datatree = xr.open_datatree(paths[0])\n",
    "dataset = xr.merge(datatree.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0053b-e1b5-4c8b-91cb-ab1fa0dbf848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeefc7e-1b33-483e-ad3b-6dba8e142a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f8330f-51ba-48f6-8dc4-461f7eafb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bda2b-50dd-4432-b172-00391479e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://seabass.gsfc.nasa.gov/wiki/seabass_tools/sb_utilities-0.0.2.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99417e95-8cb6-4b6d-81ed-e9fb39311bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List l2 flags, then build them into a dict\n",
    "l2_flags_list = [\n",
    "    \"ATMFAIL\",\n",
    "    \"LAND\",\n",
    "    \"PRODWARN\",\n",
    "    \"HIGLINT\",\n",
    "    \"HILT\",\n",
    "    \"HISATZEN\",\n",
    "    \"COASTZ\",\n",
    "    \"SPARE\",\n",
    "    \"STRAYLIGHT\",\n",
    "    \"CLDICE\",\n",
    "    \"COCCOLITH\",\n",
    "    \"TURBIDW\",\n",
    "    \"HISOLZEN\",\n",
    "    \"SPARE\",\n",
    "    \"LOWLW\",\n",
    "    \"CHLFAIL\",\n",
    "    \"NAVWARN\",\n",
    "    \"ABSAER\",\n",
    "    \"SPARE\",\n",
    "    \"MAXAERITER\",\n",
    "    \"MODGLINT\",\n",
    "    \"CHLWARN\",\n",
    "    \"ATMWARN\",\n",
    "    \"SPARE\",\n",
    "    \"SEAICE\",\n",
    "    \"NAVFAIL\",\n",
    "    \"FILTER\",\n",
    "    \"SPARE\",\n",
    "    \"BOWTIEDEL\",\n",
    "    \"HIPOL\",\n",
    "    \"PRODFAIL\",\n",
    "    \"SPARE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5438471-ee45-40e5-a9e3-a5cdda56aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_FLAGS = {flag: 1 << idx for idx, flag in enumerate(l2_flags_list)}\n",
    "\n",
    "# Bailey and Werdell 2006 exclusion criteria\n",
    "EXCLUSION_FLAGS = [\n",
    "    #\"LAND\",\n",
    "    #\"HIGLINT\",\n",
    "    #\"HILT\",\n",
    "    #\"STRAYLIGHT\",\n",
    "    \"CLDICE\",\n",
    "    #\"ATMFAIL\",\n",
    "    #\"LOWLW\",\n",
    "    #\"FILTER\",\n",
    "    #\"NAVFAIL\",\n",
    "    #\"NAVWARN\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa9525-0a83-4f45-b055-dec9faa19a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short names for earthaccess lookup\n",
    "SAT_LOOKUP = {\n",
    "    \"PACE_AOP\": \"PACE_OCI_L2_AOP\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7408394-c38e-41d6-99d4-472cd64a933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------##\n",
    "#                 Load the OCI sensor file and return F0.                     #\n",
    "##---------------------------------------------------------------------------##\n",
    "\n",
    "def get_f0(wavelengths=None, window_size=10):\n",
    "    \"\"\"Load the OCI sensor file and return F0.\n",
    "\n",
    "    Defaults to returning the full table. Input obs_time to correct for the\n",
    "    Earth-Sun distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sensor_file : str or pathlib.Path\n",
    "        Path to the OCI satellite sensor file containing wavelengths and F0.\n",
    "    wavelengths : array-like, optional\n",
    "        Wavelengths at which to compute the average irradiance.\n",
    "        If None, returns the full wavelength and irradiance table.\n",
    "    window_size : int, optional\n",
    "        Bandpass filter size for mean filtering to selected wavelengths, in nm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of np.ndarray\n",
    "        A tuple containing:\n",
    "        - f0_spectra : np.ndarray\n",
    "            The extraterrestrial solar irradiance, in uW/cm^2/nm.\n",
    "        - f0_wave : np.ndarray\n",
    "            The corresponding wavelengths, in nm.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(OCI_SENSOR_FILE, \"r\") as file_in:\n",
    "        for line in file_in:\n",
    "            if \"Nbands\" in line:\n",
    "                (key, nbands) = line.split(\"=\")\n",
    "                break\n",
    "\n",
    "    wl = np.zeros(int(nbands), dtype=float)\n",
    "    f0 = np.zeros(int(nbands), dtype=float)\n",
    "    with open(OCI_SENSOR_FILE, \"r\") as file_in:\n",
    "        for line in file_in:\n",
    "            if \"=\" in line:\n",
    "                (key, value) = line.split(\"=\")\n",
    "                if \"Lambda\" in key:\n",
    "                    idx = re.findall(r\"\\d+\", key)\n",
    "                    wvlidx = int(idx[0]) - 1\n",
    "                    wl[wvlidx] = float(value)\n",
    "                if \"F0\" in key:\n",
    "                    idx = re.findall(r\"\\d+\", key)\n",
    "                    wvlidx = int(idx[1]) - 1\n",
    "                    f0[wvlidx] = float(value)\n",
    "\n",
    "    if wavelengths is not None:\n",
    "        f0_wave = np.array(wavelengths)\n",
    "        f0_spectra = bandpass_avg(f0, wl, window_size, f0_wave)\n",
    "    else:\n",
    "        f0_wave = wl\n",
    "        f0_spectra = f0\n",
    "\n",
    "    return f0_spectra, f0_wave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169486b-b41b-4e33-a8c4-d34ea736dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------##\n",
    "#                 Apply a band-pass filter to the data.                       #\n",
    "##---------------------------------------------------------------------------##\n",
    "def bandpass_avg(\n",
    "        data,\n",
    "        input_wavelengths,\n",
    "        window_size=10,\n",
    "        target_wavelengths=None\n",
    "        ):\n",
    "    \"\"\"Apply a band-pass filter to the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        1D or 2D array containing the spectral data (samples x wavelengths).\n",
    "        If 1D, it's assumed to be a single sample.\n",
    "    input_wavelengths : np.ndarray\n",
    "        1D array of wavelength values corresponding to the columns of data.\n",
    "    window_size : int, optional\n",
    "        Size of the window to use for averaging. Default is 10 nm.\n",
    "    target_wavelengths : np.ndarray, optional\n",
    "        1D array of target wavelengths for filtered values.\n",
    "        If None, the input wavelengths are used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        1D or 2D array containing the band-pass filtered data.\n",
    "\n",
    "    \"\"\"\n",
    "    data = np.atleast_2d(data)\n",
    "    half_window = window_size / 2\n",
    "    num_samples, num_input_wavelengths = data.shape\n",
    "    if target_wavelengths is None:\n",
    "        target_wavelengths = input_wavelengths\n",
    "\n",
    "    filtered_data = np.empty((num_samples, len(target_wavelengths))) * np.nan\n",
    "\n",
    "    for idx, target_wl in enumerate(target_wavelengths):\n",
    "        start = target_wl - half_window\n",
    "        end = target_wl + half_window\n",
    "        cols_in_range = np.where(\n",
    "            (input_wavelengths >= start) & (input_wavelengths <= end)\n",
    "        )[0]\n",
    "        if cols_in_range.size > 0:\n",
    "            filtered_data[:, idx] = np.nanmean(data[:, cols_in_range], axis=1)\n",
    "\n",
    "    return filtered_data if num_samples > 1 else filtered_data.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bea90b-6d99-4b9d-8de9-3b561a3e754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------##\n",
    "#         Process a dataframe to create a dictionary of data products.        #\n",
    "##---------------------------------------------------------------------------##\n",
    "\n",
    "def get_column_prods(df, type_prefix):\n",
    "    \"\"\"Process a dataframe to create a dictionary of data products.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        Extracted dataframes from read_extract_file\n",
    "    type_prefix : str\n",
    "        Prefix to identify the product columns, e.g. \"aoc\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_dict\n",
    "        dictionary mapping data product with their wavelengths and columns.\n",
    "\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    pattern = rf\"{type_prefix}_(\\w+?)(\\d*\\.?\\d+)?$\"\n",
    "\n",
    "    for col in df.columns:\n",
    "        match = re.match(pattern, col)\n",
    "        if match:\n",
    "            product = match.group(1)\n",
    "            wavelength = match.group(2) if match.group(2) else None\n",
    "            if product not in data_dict:\n",
    "                data_dict[product] = {\"wavelengths\": [], \"columns\": []}\n",
    "            data_dict[product][\"columns\"].append(col)\n",
    "            if wavelength:\n",
    "                if \".\" in wavelength:\n",
    "                    data_dict[product][\"wavelengths\"].append(float(wavelength))\n",
    "                else:\n",
    "                    data_dict[product][\"wavelengths\"].append(int(wavelength))\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e812c-22ce-45b1-9035-b43127b9814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------##\n",
    "#                Read SeaBASS file and returns just the data.                 #\n",
    "##---------------------------------------------------------------------------##\n",
    "\n",
    "import pandas as pd\n",
    "import builtins\n",
    "\n",
    "def read_sb(filename_sb):\n",
    "    \"\"\"Read SeaBASS .sb file, parse header and data, \n",
    "       then attach profile_lat, profile_lon, profile_time.\"\"\"\n",
    "    # 1) Load all lines\n",
    "    with builtins.open(filename_sb, \"r\") as f:\n",
    "        lines = [l.rstrip(\"\\n\") for l in f]\n",
    "\n",
    "    # 2) Find where the header ends\n",
    "    endh = next(i for i, L in enumerate(lines) if L == \"/end_header\")\n",
    "\n",
    "    # 3) Parse header into a dict, but only lines with \"/\" **and** \"=\"\n",
    "    headers = {}\n",
    "    for line in lines[:endh]:\n",
    "        if not line.startswith(\"/\") or \"=\" not in line:\n",
    "            continue\n",
    "        key, val = line[1:].split(\"=\", 1)  # strip \"/\" then split\n",
    "        headers[key] = val\n",
    "\n",
    "    # 4) Read the CSV portion into a DataFrame\n",
    "    df = pd.read_csv(\n",
    "        filename_sb,\n",
    "        skiprows=endh + 1,\n",
    "        names=headers[\"fields\"].split(\",\"),\n",
    "        na_values=headers.get(\"missing\", \"\")\n",
    "    )\n",
    "\n",
    "    # 5) Build the datetime index (your existing routine)\n",
    "    get_sb_datetime(df)\n",
    "\n",
    "    # 6) Extract & clean metadata from headers\n",
    "    #    Strip off any \"[...]\" before converting to float\n",
    "    lat_str = headers[\"north_latitude\"].split(\"[\", 1)[0]\n",
    "    lon_str = headers[\"east_longitude\"].split(\"[\", 1)[0]\n",
    "    lat = float(lat_str)\n",
    "    lon = float(lon_str)\n",
    "\n",
    "    #    Strip \"[GMT]\" from the time field\n",
    "    time_str = headers[\"start_time\"].split(\"[\", 1)[0]\n",
    "    dt0 = pd.to_datetime(headers[\"start_date\"] + \" \" + time_str)\n",
    "\n",
    "    # 7) Attach them as new columns on every row\n",
    "    df[\"profile_lat\"]  = lat\n",
    "    df[\"profile_lon\"]  = lon\n",
    "    df[\"profile_time\"] = dt0\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca28eb-fc23-4d19-8744-12d2cdef14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------##\n",
    "#     Parse datetime from different combinations of dates and times.          #\n",
    "##---------------------------------------------------------------------------##\n",
    "\n",
    "def get_sb_datetime(df):\n",
    "    \"\"\"Parse datetime from different combinations of dates and times.\"\"\"\n",
    "    if all(col in df.columns for col in [\"year\", \"month\", \"day\",\n",
    "                                         \"hour\", \"minute\", \"second\"]):\n",
    "        df[\"datetime\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\",\n",
    "                                            \"hour\", \"minute\", \"second\"]])\n",
    "    elif all(col in df.columns for col in [\"year\", \"month\", \"day\", \"time\"]):\n",
    "        df[\"datetime\"] = pd.to_datetime(\n",
    "            df[\"year\"].astype(str) + df[\"month\"].astype(str).str.zfill(2)\n",
    "            + df[\"day\"].astype(str).str.zfill(2) + ' ' + df[\"time\"])\n",
    "    elif all(col in df.columns for col in [\"date\", \"time\"]):\n",
    "        df[\"datetime\"] = pd.to_datetime(\n",
    "            df[\"date\"].astype(str) + ' ' + df[\"time\"])\n",
    "    elif all(col in df.columns for col in [\"year\", \"month\", \"day\"]):\n",
    "        df[\"datetime\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "    elif all(col in df.columns for col in [\"date\", \"hour\",\n",
    "                                           \"minute\", \"second\"]):\n",
    "        df[\"datetime\"] = pd.to_datetime(\n",
    "            df[\"date\"].astype(str) + ' ' + df[\"hour\"].astype(str).str.zfill(2)\n",
    "            + ':' + df[\"minute\"].astype(str).str.zfill(2) + ':'\n",
    "            + df[\"second\"].astype(str).str.zfill(2))\n",
    "    else:\n",
    "        print(\"Unrecognized date/time format in DataFrame columns.\"\n",
    "              \"\\nMay be a profile, but doublecheck.\")\n",
    "        return\n",
    "\n",
    "    # Reindex the dataframe with the new datetime\n",
    "    df.set_index(\"datetime\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687b631-3910-4ccf-b698-8ddf492171bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file_path = '/home/jovyan/shared-public/pace-hackweek/SeePACE/'\n",
    "file_path += 'Hackweek_PACE-PAX_Rrs/NRL/PACE-PAX/'\n",
    "file_path += 'PACE-PAX_Shearwater/archive/'\n",
    "file_path += 'PVST_POL_PACE-PAX_Shearwater_above_water_radiometry_nflh_NRL_20240906_St_1_R1.sb'\n",
    "df = read_sb(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163cbdf-4748-45bf-8a08-2bde6c7d4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume df is your 726×9 DataFrame with a “Wavelength” column\n",
    "df_wide = df.set_index(\"Wavelength\").T\n",
    "\n",
    "print(df_wide.shape)   # → (9, 726)\n",
    "df_wide.index.name = None          # drop the index name if you like\n",
    "df_wide.columns.name = \"λ (nm)\"    # optional: name the wavelength axis\n",
    "\n",
    "df_wide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2146008-a406-4888-93c9-c559c577e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Point this at your “archive” folder containing all the .sb files\n",
    "archive_dir = \"/home/jovyan/shared-public/pace-hackweek/SeePACE/\" \\\n",
    "            + \"Hackweek_PACE-PAX_Rrs/NRL/PACE-PAX/PACE-PAX_Shearwater/archive\"\n",
    "\n",
    "# 2) Grab a sorted list of all the .sb paths\n",
    "sb_files = sorted(glob.glob(os.path.join(archive_dir, \"*.sb\")))\n",
    "\n",
    "# 3) Loop over them, reading each one and collecting into a list\n",
    "df_list = []\n",
    "for sb_path in sb_files:\n",
    "    df = read_sb(sb_path)          # your metadata‐aware reader\n",
    "    df_list.append(df)\n",
    "\n",
    "# 4) Stack them into a single DataFrame\n",
    "all_profiles = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 5) Inspect\n",
    "print(all_profiles.shape)   # → (number_of_profiles*726, number_of_columns)\n",
    "all_profiles.head()\n",
    "\n",
    "# 6) (Optional) Save to disk for fast reload later\n",
    "all_profiles.to_csv(\"all_SeaBASS_profiles.csv\", index=False)\n",
    "# or\n",
    "all_profiles.to_pickle(\"all_SeaBASS_profiles.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c92d95-311e-45d6-90fe-810b2b9c4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# From CSV (human‐readable, but a little slower to load)\n",
    "df_csv = pd.read_csv(\"all_SeaBASS_profiles.csv\")\n",
    "df_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35569f78-67c9-427b-8e61-0599f9d9a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume df is your 26862×12 DataFrame loaded from CSV/pickle\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Load your combined SeaBASS CSV (set the correct path/filename)\n",
    "df = pd.read_csv(\"all_SeaBASS_profiles.csv\")\n",
    "\n",
    "# 1) Pivot so each profile (profile_time, lat, lon) becomes one row,\n",
    "#    and each Wavelength becomes its own column holding the Rrs value.\n",
    "wide = df.pivot(\n",
    "    index=[\"profile_time\", \"profile_lat\", \"profile_lon\"],\n",
    "    columns=\"Wavelength\",\n",
    "    values=\"Rrs\"\n",
    ")\n",
    "\n",
    "# 2) Turn the pivot index back into columns and rename them:\n",
    "wide = (\n",
    "    wide\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        \"profile_time\":\"datetime\",\n",
    "        \"profile_lat\":\"lat\",\n",
    "        \"profile_lon\":\"lon\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# 3) Convert 'datetime' to real Timestamp and split out date & time strings:\n",
    "wide[\"datetime\"] = pd.to_datetime(wide[\"datetime\"])\n",
    "wide[\"date\"]     = wide[\"datetime\"].dt.strftime(\"%Y%m%d\")\n",
    "wide[\"time\"]     = wide[\"datetime\"].dt.strftime(\"%H:%M:%S\")\n",
    "\n",
    "# 4) Reorder: metadata first, then wavelengths in ascending order\n",
    "wls = sorted(c for c in wide.columns if isinstance(c, (int, float)))\n",
    "wide = wide[[\"datetime\", \"date\", \"time\", \"lat\", \"lon\"] + wls]\n",
    "\n",
    "# 5) Inspect\n",
    "print(wide.shape)   # → (number_of_profiles, 5 + number_of_wavelengths)\n",
    "wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68a166-965e-4b9b-aa64-da88e58fbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (re)build the wide table once and for all:\n",
    "df = pd.read_csv(\"all_SeaBASS_profiles.csv\", parse_dates=[\"profile_time\"])\n",
    "df_wide = df.pivot(\n",
    "    index=[\"profile_time\",\"profile_lat\",\"profile_lon\"],\n",
    "    columns=\"Wavelength\",\n",
    "    values=\"Rrs\"\n",
    ").reset_index().rename(columns={\n",
    "    \"profile_time\":\"datetime\",\n",
    "    \"profile_lat\":  \"lat\",\n",
    "    \"profile_lon\":  \"lon\"\n",
    "})\n",
    "# split out date/time if you need them\n",
    "df_wide[\"date\"] = df_wide[\"datetime\"].dt.strftime(\"%Y%m%d\")\n",
    "df_wide[\"time\"] = df_wide[\"datetime\"].dt.strftime(\"%H:%M:%S\")\n",
    "# reorder columns\n",
    "wls = sorted(c for c in df_wide.columns if isinstance(c,(int,float)))\n",
    "df = df_wide[[\"datetime\",\"date\",\"time\",\"lat\",\"lon\"] + wls]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32380c73-471b-4dc9-b3e0-2114174ab87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------##\n",
    "#                             Satellite Utilities                             #\n",
    "##---------------------------------------------------------------------------##\n",
    "def parse_quality_flags(flag_value):\n",
    "    \"\"\"Parse bitwise flag into a list of flag names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    flag_value : int\n",
    "        The integer representing the combined bitwise quality flags.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        List of flag names that are set in the flag_value.\n",
    "\n",
    "    \"\"\"\n",
    "    return [\n",
    "        flag_name for flag_name, value in L2_FLAGS.items()\n",
    "        if (flag_value & value) != 0\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbee7c9-7dbd-4ffb-a32d-7fa2a492b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fivebyfive(file, latitude, longitude, rrs_wavelengths):\n",
    "    \"\"\"Get stats on 5x5 box around station coordinates of a satellite granule.\n",
    "\n",
    "    This checks l2flags and runs statistics on valid pixels and returns their\n",
    "    valid count, the coefficient of variance (cv), and the Rrs values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : earthaccess granule object\n",
    "        Satellite granule from earthaccess.\n",
    "    latitude : float\n",
    "        In decimal degrees for Aeronet-OC site for matchups\n",
    "    longitude : float\n",
    "        In decimal degrees (negative West) for Aeronet-OC site for matchups\n",
    "    rrs_wavelengths ; numpy array\n",
    "        Rrs wavelengths (from wavelength_3d for OCI)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of the processed 5x5 box with:\n",
    "            - \"sat_datetime\": pd.datetime\n",
    "                Datetime of the overall granule start time\n",
    "            - \"sat_cv\": float\n",
    "                Median coefficient of variation of Rrs(405nm - 570nm)\n",
    "            - \"sat_latitude\": float\n",
    "                Latitude of center pixel\n",
    "            - \"sat_longitude\": float\n",
    "                Longitude of center pixel\n",
    "            - \"sat_pixel_valid\": float\n",
    "                Number of valid pixels in 5x5 box based on l2 flags\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This is set to use just Rrs data for the demo. As an exercise, make this\n",
    "    function more generalized by adding an input for the desired product and\n",
    "    removing the wavelength dependency (if not needed) as well as the cv\n",
    "    calculation. This will also require refactoring the `match_data` function.\n",
    "    \"\"\"\n",
    "    with xr.open_dataset(file, group=\"navigation_data\") as ds_nav:\n",
    "        sat_lat = ds_nav[\"latitude\"].values\n",
    "        sat_lon = ds_nav[\"longitude\"].values\n",
    "\n",
    "    # Calculate the Euclidean distance for 2D lat/lon arrays\n",
    "    distances = np.sqrt((sat_lat - latitude) ** 2 + (sat_lon - longitude) ** 2)\n",
    "\n",
    "    # Find the index of the minimum distance\n",
    "    # Dimensions are (lines, pixels)\n",
    "    min_dist_idx = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "    center_line, center_pixel = min_dist_idx\n",
    "\n",
    "    # Get indices for a 5x5 box around the center pixel\n",
    "    line_start = max(center_line - 2, 0)\n",
    "    line_end = min(center_line + 2 + 1, sat_lat.shape[0])\n",
    "    pixel_start = max(center_pixel - 2, 0)\n",
    "    pixel_end = min(center_pixel + 2 + 1, sat_lat.shape[1])\n",
    "\n",
    "    # Extract the data\n",
    "    # NOTE: This is hard-coded to Rrs from an L2 AOP file.\n",
    "    with xr.open_dataset(file, group=\"geophysical_data\") as ds_data:\n",
    "        rrs_data = (\n",
    "            ds_data[\"Rrs\"].isel(\n",
    "                number_of_lines=slice(line_start, line_end),\n",
    "                pixels_per_line=slice(pixel_start, pixel_end),\n",
    "            ).values\n",
    "        )\n",
    "        flags_data = (\n",
    "            ds_data[\"l2_flags\"].isel(\n",
    "                number_of_lines=slice(line_start, line_end),\n",
    "                pixels_per_line=slice(pixel_start, pixel_end),\n",
    "            ).values\n",
    "        )\n",
    "\n",
    "    # Calculate the bitwise OR of all flags in EXCLUSION_FLAGS to get a mask\n",
    "    exclude_mask = sum(L2_FLAGS[flag] for flag in EXCLUSION_FLAGS)\n",
    "\n",
    "    # Create a boolean mask\n",
    "    # True means the flag value does not contain any of the EXCLUSION_FLAGS\n",
    "    valid_mask = np.bitwise_and(flags_data, exclude_mask) == 0\n",
    "\n",
    "    # Get stats and averages\n",
    "    if valid_mask.any():\n",
    "        rrs_valid = rrs_data[valid_mask]\n",
    "        rrs_std_initial = np.std(rrs_valid, axis=0)\n",
    "        rrs_mean_initial = np.mean(rrs_valid, axis=0)\n",
    "\n",
    "        # Exclude spectra > 1.5 stdevs away\n",
    "        std_mask = np.all(\n",
    "            np.abs(rrs_valid - rrs_mean_initial) <= 1.5 * rrs_std_initial,\n",
    "            axis=1\n",
    "        )\n",
    "        rrs_std = np.std(rrs_valid[std_mask], axis=0)\n",
    "        rrs_mean = np.mean(rrs_valid[std_mask], axis=0).flatten()\n",
    "\n",
    "        # Matchup criteria uses cv as median of 405-570nm\n",
    "        rrs_cv = rrs_std / rrs_mean\n",
    "        rrs_cv_median = np.median(\n",
    "            rrs_cv[(rrs_wavelengths >= 405) & (rrs_wavelengths <= 570)]\n",
    "        )\n",
    "    else:\n",
    "        rrs_cv_median = np.nan\n",
    "        rrs_mean = np.nan * np.empty_like(rrs_wavelengths)\n",
    "\n",
    "    # Put in dictionary of the row\n",
    "    row = {\n",
    "        \"sat_datetime\": pd.to_datetime(\n",
    "            file.granule[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"],\n",
    "            utc=0\n",
    "        ),\n",
    "        \"sat_cv\": rrs_cv_median,\n",
    "        \"sat_latitude\": sat_lat[center_line, center_pixel],\n",
    "        \"sat_longitude\": sat_lon[center_line, center_pixel],\n",
    "        \"sat_pixel_valid\": np.sum(valid_mask),\n",
    "    }\n",
    "\n",
    "    # Add mean spectra to the row dictionary\n",
    "    for wavelength, mean_value in zip(rrs_wavelengths, rrs_mean):\n",
    "        key = f\"sat_rrs{int(wavelength)}\"\n",
    "        row[key] = mean_value\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e948b8f-5160-4f96-a92d-6b9792c8dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not use this. \n",
    "\n",
    "\n",
    "def get_sat_ts_matchups(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    latitude,\n",
    "    longitude,\n",
    "    sat=\"PACE_AOP\",\n",
    "    selected_dates=None\n",
    "):\n",
    "    \"\"\"Make satellite timeseries of matchups from single station.\n",
    "\n",
    "    Caution: If the date or coordinates aren't formatted correctly, it might\n",
    "    pull a huge granule list and take forever to run. If it takes more than 45\n",
    "    seconds to print the number of granules, just kill the process.\n",
    "\n",
    "    Uses the earthaccess package. Defaults to the PACE OCI L2 IOP datasets,\n",
    "    but other satellites can be used if they have a corresponding short_name\n",
    "    in the SAT_LOOKUP dictionary.\n",
    "\n",
    "    Workflow:\n",
    "        1. Get list of matchup granules\n",
    "        2. Loop through each file and:\n",
    "            2a. Find closest pixel to station, extract 5x5 pixel box\n",
    "            2b. Exclude pixels based on l2_flags\n",
    "            2c. Filtered mean to get single spectra\n",
    "            2d. Compute statistics and save data row\n",
    "        3. Organize output pandas dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_date : datetime or str\n",
    "        Beginning of Aeronet data to run.\n",
    "    end_date : datetime or str, optional\n",
    "        End of Aeronet data to run.\n",
    "    latitude : float\n",
    "        In decimal degrees for Aeronet-OC site for matchups\n",
    "    longitude : float\n",
    "        In decimal degrees (negative West) for Aeronet-OC site for matchups\n",
    "    sat : str\n",
    "        Name of satellite to search. Must be in SAT_LOOKUP dict constant.\n",
    "    selected_dates : list of str, optional\n",
    "        If given, only pull granules if the dates are in this list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame object\n",
    "        Flattened table of all satellite granule matchups.\n",
    "\n",
    "    \"\"\"\n",
    "    # Look up short name from constants\n",
    "    if sat not in SAT_LOOKUP.keys():\n",
    "        raise ValueError(\n",
    "            f\"{sat} is not in the lookup dictionary. Available \"\n",
    "            f\"sats are: {', '.join(SAT_LOOKUP)}\"\n",
    "        )\n",
    "    short_name = SAT_LOOKUP[sat]\n",
    "\n",
    "    # Format search parameters\n",
    "    time_bounds = (f\"{start_date}T00:00:00\", f\"{end_date}T23:59:59\")\n",
    "\n",
    "    # Run Earthaccess data search\n",
    "    results = earthaccess.search_data(\n",
    "        point=(longitude, latitude),\n",
    "        temporal=time_bounds,\n",
    "        short_name=short_name\n",
    "    )\n",
    "    if selected_dates is not None:\n",
    "        filtered_results = [\n",
    "            result\n",
    "            for result in results\n",
    "            if result[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"][:10]\n",
    "            in selected_dates\n",
    "        ]\n",
    "        print(f\"Filtered to {len(filtered_results)} Granules.\")\n",
    "        files = earthaccess.open(filtered_results)\n",
    "    else:\n",
    "        files = earthaccess.open(results)\n",
    "\n",
    "    # Pull out Rrs wavelengths for easier processing\n",
    "    with xr.open_dataset(files[0], group=\"sensor_band_parameters\") as ds_bands:\n",
    "        rrs_wavelengths = ds_bands[\"wavelength_3d\"].values\n",
    "\n",
    "    # Loop through files and process\n",
    "    sat_rows = []\n",
    "    for idx, file in enumerate(files):\n",
    "        granule_date = pd.to_datetime(\n",
    "            file.granule[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "        )\n",
    "        print(f\"Running Granule: {granule_date}\")\n",
    "        row = get_fivebyfive(file, latitude, longitude, rrs_wavelengths)\n",
    "        sat_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(sat_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8381ce9-9877-4734-9afa-23dcce38f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New match up data (We do not use this just for test)\n",
    "import pandas as pd\n",
    "\n",
    "def match_data(\n",
    "    df_sat,\n",
    "    df_field,\n",
    "    cv_max=0.15,\n",
    "    senz_max=60.0,\n",
    "    min_percent_valid=55.0,\n",
    "    max_time_diff=180,\n",
    "    std_max=1.5,\n",
    "):\n",
    "    \"\"\"Create matchup dataframe based on selection criteria.\"\"\"\n",
    "    # Setup\n",
    "    time_window = pd.Timedelta(minutes=max_time_diff)\n",
    "    df_match_list = []\n",
    "\n",
    "    # (Optionally filter field by solar zenith)\n",
    "    df_field_filtered = df_field.copy()\n",
    "\n",
    "    # 1) Pull the datetime (may be tz-aware) out of the index\n",
    "    df_field_filtered[\"field_datetime\"] = df_field_filtered.index\n",
    "    # ← added: make sure field_datetime is tz-naive\n",
    "    df_field_filtered[\"field_datetime\"] = (\n",
    "        pd.to_datetime(df_field_filtered[\"field_datetime\"])\n",
    "          .dt.tz_localize(None)\n",
    "    )  # ← added\n",
    "\n",
    "    # 2) Rename lat/lon into the names used below  \n",
    "    df_field_filtered[\"field_latitude\"]  = df_field_filtered[\"lat\"]  \n",
    "    df_field_filtered[\"field_longitude\"] = df_field_filtered[\"lon\"]\n",
    "\n",
    "    # Filter satellite data based on cv threshold\n",
    "    df_sat_filtered = df_sat[df_sat[\"sat_cv\"] <= cv_max] \n",
    "    # Filter satellite data based on percent good pixels\n",
    "    df_sat_filtered = df_sat_filtered[\n",
    "        df_sat_filtered[\"sat_pixel_valid\"] >= min_percent_valid * 25 / 100\n",
    "    ]\n",
    "\n",
    "    for _, sat_row in df_sat_filtered.iterrows():\n",
    "        # 1) Strip the UTC tag off the sat timestamp:\n",
    "        sat_time = sat_row[\"sat_datetime\"].tz_convert(None)\n",
    "\n",
    "        # 2) Now subtract your field datetimes (which are tz-naive) from that:\n",
    "        time_diff = abs(df_field_filtered[\"field_datetime\"] - sat_time)\n",
    "\n",
    "        # 3) Continue with your masks:\n",
    "        time_mask = time_diff <= time_window\n",
    "        lat_mask  = abs(df_field_filtered[\"field_latitude\"] - sat_row[\"sat_latitude\"]) <= 0.2\n",
    "        lon_mask  = abs(df_field_filtered[\"field_longitude\"] - sat_row[\"sat_longitude\"]) <= 0.2\n",
    "\n",
    "        field_matches = df_field_filtered[time_mask & lat_mask & lon_mask]\n",
    "\n",
    "        if field_matches.shape[0] > 5:\n",
    "            # Filter by Standard Deviation for rrs columns\n",
    "            rrs_cols = [\n",
    "                col for col in field_matches.columns\n",
    "                if col.startswith(\"field_rrs\")\n",
    "                and 400 <= int(col.rsplit(\"_rrs\")[1]) <= 700\n",
    "            ]\n",
    "            if rrs_cols:\n",
    "                mean_spectra = field_matches[rrs_cols].mean(axis=0)\n",
    "                std_spectra  = field_matches[rrs_cols].std(axis=0)\n",
    "                within_std   = (\n",
    "                    abs(field_matches[rrs_cols] - mean_spectra) \n",
    "                    <= std_max * std_spectra\n",
    "                )\n",
    "                field_matches = field_matches[within_std.all(axis=1)]\n",
    "\n",
    "        if not field_matches.empty:\n",
    "            # Select the best match based on time delta\n",
    "            time_diff   = abs(\n",
    "                field_matches[\"field_datetime\"] - sat_row[\"sat_datetime\"]\n",
    "            )\n",
    "            best_match = field_matches.loc[time_diff.idxmin()]\n",
    "            df_match_list.append({**best_match.to_dict(), **sat_row.to_dict()})\n",
    "\n",
    "    df_match = pd.DataFrame(df_match_list)\n",
    "    return df_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f549723-380e-400a-93fd-bdcd40cc2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the first station’s coords out of your SeaBASS/field DataFrame\n",
    "station_lat = df[\"lat\"].iloc[0]\n",
    "station_lon = df[\"lon\"].iloc[0]\n",
    "\n",
    "# suppose your datetime column is called 'field_datetime'\n",
    "#unique_days = df.index.date             # e.g. array([datetime.date(2024,9,22), ...])\n",
    "#unique_days_str = sorted({d.strftime(\"%Y-%m-%d\") for d in unique_days}) # e.g. ['2024-09-22', '2024-09-23', ...]\n",
    "# now call the satellite‐matchup routine with just those floats\n",
    "#df_satellite = get_sat_ts_matchups(\n",
    "   # start_date=\"2024-03-01\",\n",
    "   # end_date=\"2024-03-05\",\n",
    "   # latitude=station_lat,    # e.g. 34.2163\n",
    "    #longitude=station_lon,    # e.g. -119.5980\n",
    "   # sat=\"PACE_AOP\",            # only if you want to override the default\n",
    "   # selected_dates=unique_days_str\n",
    "#)\n",
    "\n",
    "df_satellite = get_sat_ts_matchups(\n",
    "    start_date= \"2024-09-26\",\n",
    "    end_date=   \"2024-09-26\",\n",
    "    latitude=   station_lat,\n",
    "    longitude=  station_lon,\n",
    "    sat=\"PACE_AOP\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128de08-33dc-40fd-8a30-feedd95248d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchups = match_data(\n",
    "    df_satellite,   # <-- DataFrame of sat rows, not the metadata list\n",
    "    df,             # your in-situ SeaBASS DataFrame\n",
    "    cv_max=0.60,\n",
    "    senz_max=60.0,\n",
    "    min_percent_valid=55.0,\n",
    "    max_time_diff=240,\n",
    "    std_max=1.5,\n",
    ")\n",
    "matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05defc9e-3c4b-412c-b565-403c69c2e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final matchup file\n",
    "import pandas as pd\n",
    "\n",
    "def match_data(\n",
    "    df_sat,\n",
    "    df_field,\n",
    "    cv_max=0.15,\n",
    "    senz_max=60.0,\n",
    "    min_percent_valid=55.0,\n",
    "    max_time_diff=180,\n",
    "    std_max=1.5,\n",
    "):\n",
    "    \"\"\"Create matchup dataframe based on selection criteria.\"\"\"\n",
    "    time_window   = pd.Timedelta(minutes=max_time_diff)\n",
    "    df_match_list = []\n",
    "\n",
    "    # 1) prepare your field table\n",
    "    df_field_filtered = df_field.copy()\n",
    "\n",
    "    # pull real datetimes out of the index\n",
    "    df_field_filtered[\"field_datetime\"] = df_field_filtered.index\n",
    "    # ensure tz-naive\n",
    "    df_field_filtered[\"field_datetime\"] = (\n",
    "        pd.to_datetime(df_field_filtered[\"field_datetime\"])\n",
    "          .dt.tz_localize(None)\n",
    "    )\n",
    "\n",
    "    # rename lat/lon\n",
    "    df_field_filtered[\"field_latitude\"]  = df_field_filtered[\"lat\"]\n",
    "    df_field_filtered[\"field_longitude\"] = df_field_filtered[\"lon\"]\n",
    "\n",
    "    # 2) filter satellite rows\n",
    "    df_sat_filtered = df_sat[df_sat[\"sat_cv\"] <= cv_max]\n",
    "    df_sat_filtered = df_sat_filtered[\n",
    "        df_sat_filtered[\"sat_pixel_valid\"] >= min_percent_valid * 25/100\n",
    "    ]\n",
    "\n",
    "    for _, sat_row in df_sat_filtered.iterrows():\n",
    "        # ---- HIGHLIGHTED: drop tz once, store in sat_time ----\n",
    "        sat_time = sat_row[\"sat_datetime\"].tz_convert(None)\n",
    "\n",
    "        # first masking by time\n",
    "        time_diff = abs(df_field_filtered[\"field_datetime\"] - sat_time)\n",
    "        time_mask = time_diff <= time_window\n",
    "\n",
    "        lat_mask = abs(df_field_filtered[\"field_latitude\"] - sat_row[\"sat_latitude\"]) <= 0.2\n",
    "        lon_mask = abs(df_field_filtered[\"field_longitude\"] - sat_row[\"sat_longitude\"]) <= 0.2\n",
    "\n",
    "        field_matches = df_field_filtered[time_mask & lat_mask & lon_mask]\n",
    "\n",
    "        # apply your stdev filtering if you like...\n",
    "        if field_matches.shape[0] > 5:\n",
    "            rrs_cols = [\n",
    "                c for c in field_matches.columns\n",
    "                if c.startswith(\"field_rrs\")\n",
    "                and 400 <= int(c.rsplit(\"_rrs\")[1]) <= 700\n",
    "            ]\n",
    "            if rrs_cols:\n",
    "                mean_spectra = field_matches[rrs_cols].mean(axis=0)\n",
    "                std_spectra  = field_matches[rrs_cols].std(axis=0)\n",
    "                mask         = (\n",
    "                    abs(field_matches[rrs_cols] - mean_spectra)\n",
    "                    <= std_max * std_spectra\n",
    "                )\n",
    "                field_matches = field_matches[mask.all(axis=1)]\n",
    "\n",
    "        if not field_matches.empty:\n",
    "            # ---- HIGHLIGHTED: use the same tz-naive sat_time here ----\n",
    "            time_diff   = abs(field_matches[\"field_datetime\"] - sat_time)\n",
    "            best_match  = field_matches.loc[time_diff.idxmin()]\n",
    "            df_match_list.append({**best_match.to_dict(), **sat_row.to_dict()})\n",
    "\n",
    "    return pd.DataFrame(df_match_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3211f-6e73-4fcb-8135-00511b55d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code only for one date as test\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Rebuild the wide in-situ table\n",
    "# -------------------------------\n",
    "# (a) load the long‐form CSV (one row per λ)\n",
    "df_long = pd.read_csv(\"all_SeaBASS_profiles.csv\",\n",
    "                      parse_dates=[\"profile_time\"])\n",
    "\n",
    "# (b) pivot to one row per cast, columns=Rrs at each λ\n",
    "df_wide = (\n",
    "    df_long\n",
    "    .pivot(index=[\"profile_time\",\"profile_lat\",\"profile_lon\"],\n",
    "           columns=\"Wavelength\",\n",
    "           values=\"Rrs\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        \"profile_time\":\"datetime\",\n",
    "        \"profile_lat\":\"lat\",\n",
    "        \"profile_lon\":\"lon\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# (c) optional: split date & time\n",
    "df_wide[\"date\"] = df_wide[\"datetime\"].dt.strftime(\"%Y%m%d\")\n",
    "df_wide[\"time\"] = df_wide[\"datetime\"].dt.strftime(\"%H:%M:%S\")\n",
    "\n",
    "# (d) reorder so meta first, then numeric λ in ascending order\n",
    "wls     = sorted(c for c in df_wide.columns if isinstance(c,(int,float)))\n",
    "df_wide = df_wide[[\"datetime\",\"date\",\"time\",\"lat\",\"lon\"] + wls]\n",
    "\n",
    "# confirm\n",
    "print(df_wide.shape)          \n",
    "print(df_wide.columns.tolist())  \n",
    "# → ['datetime','date','time','lat','lon',350,351,…,1075]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Make sure datetime is the index\n",
    "# -------------------------------\n",
    "df_wide = df_wide.set_index(\"datetime\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Run your satellite‐matchup\n",
    "# -------------------------------\n",
    "# pull your station coords from the very first cast\n",
    "station_lat = df_wide[\"lat\"].iloc[0]\n",
    "station_lon = df_wide[\"lon\"].iloc[0]\n",
    "\n",
    "# fetch the PACE_AOP series at that location & date\n",
    "df_satellite = get_sat_ts_matchups(\n",
    "    start_date=\"2024-09-26\",\n",
    "    end_date  =\"2024-09-26\",\n",
    "    latitude  = station_lat,\n",
    "    longitude = station_lon,\n",
    "    sat       =\"PACE_AOP\"\n",
    ")\n",
    "\n",
    "# now call your existing match_data\n",
    "matchups = match_data(\n",
    "    df_satellite,\n",
    "    df_wide,\n",
    "    cv_max            = 0.60,\n",
    "    senz_max          = 60.0,\n",
    "    min_percent_valid = 55.0,\n",
    "    max_time_diff     = 240,\n",
    "    std_max           = 1.5,\n",
    ")\n",
    "\n",
    "matchups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16069ded-0973-4406-aa29-678f6328a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) we assume df_wide already exists and has index=datetime, plus lat & lon columns\n",
    "#    if you need to rebuild it, see the prior pivot code.\n",
    "\n",
    "# pull your station coords (they're constant for these casts)\n",
    "station_lat = df_wide[\"lat\"].iloc[0]\n",
    "station_lon = df_wide[\"lon\"].iloc[0]\n",
    "\n",
    "# 2) collect the unique days you sampled\n",
    "unique_days = sorted({ts.strftime(\"%Y-%m-%d\") for ts in df_wide.index})\n",
    "print(\"Sampling days:\", unique_days)\n",
    "\n",
    "# 3) grab **all** PACE granules on those days at your lat/lon\n",
    "df_sat_all = get_sat_ts_matchups(\n",
    "    start_date     = unique_days[0],\n",
    "    end_date       = unique_days[-1],\n",
    "    latitude       = station_lat,\n",
    "    longitude      = station_lon,\n",
    "    sat            = \"PACE_AOP\",\n",
    "    selected_dates = unique_days\n",
    ")\n",
    "\n",
    "print(\"Found\", len(df_sat_all), \"satellite rows across all days.\")\n",
    "\n",
    "# 4) now do the full matchup pass in one go\n",
    "matchups_all = match_data(\n",
    "    df_sat_all,\n",
    "    df_wide,\n",
    "    cv_max            = 0.60,\n",
    "    senz_max          = 60.0,\n",
    "    min_percent_valid = 55.0,\n",
    "    max_time_diff     = 240,\n",
    "    std_max           = 1.5,\n",
    ")\n",
    "\n",
    "print(\"Got\", len(matchups_all), \"total matchups:\")\n",
    "matchups_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c43958-ed4e-406f-a992-98d21e8bd802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write it out\n",
    "matchups_all.to_csv(\"matchups_all.csv\", index=False)\n",
    "print(\"Saved to\", os.path.abspath(\"matchups_all.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c07fd-6445-4251-879f-f98a70674c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c71bc7-a9d4-4b9c-8336-c71fc9e88411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load\n",
    "df = pd.read_csv(\"matchups_all.csv\", parse_dates=[\"date\", \"time\"])\n",
    "\n",
    "# 2. Combine date & time into a single datetime index\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"date\"].dt.strftime(\"%Y-%m-%d\") + \" \" +\n",
    "                                df[\"time\"].dt.strftime(\"%H:%M:%S\"))\n",
    "df.set_index(\"datetime\", inplace=True)\n",
    "\n",
    "# 3. Identify your wavelengths\n",
    "#    in your file, the in-situ columns are \"350\", \"351\", …, \"719\"\n",
    "#    and the satellite means are \"sat_rrs350\", \"sat_rrs351\", …, \"sat_rrs719\"\n",
    "wls = sorted({int(c) for c in df.columns if c.isdigit()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c5eba-743b-4395-8c99-f11eb6ddcd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Load your CSV\n",
    "#    If “date” is YYYYMMDD as int, read as string so we can parse a format\n",
    "df = pd.read_csv(\"matchups_all.csv\", dtype={\"date\": str})\n",
    "\n",
    "# 2) Build a proper datetime index\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\")\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"date\"].dt.strftime(\"%Y-%m-%d\") + \" \" + df[\"time\"])\n",
    "df.set_index(\"datetime\", inplace=True)\n",
    "\n",
    "# 3) Identify your in-situ vs. satellite columns\n",
    "insitu_cols = [c for c in df.columns if c.isdigit()]\n",
    "sat_cols   = [c for c in df.columns if c.startswith(\"sat_rrs\")]\n",
    "\n",
    "# 4) Strip off the “sat_rrs” prefix and intersect\n",
    "sat_wls     = [c.replace(\"sat_rrs\", \"\") for c in sat_cols]\n",
    "common_wls  = sorted(int(w) for w in set(insitu_cols).intersection(sat_wls))\n",
    "\n",
    "print(\"Will plot these wavelengths:\", common_wls)\n",
    "\n",
    "# 5) Loop over each common wavelength\n",
    "for wl in common_wls:\n",
    "    insitu_col = str(wl)\n",
    "    sat_col    = f\"sat_rrs{wl}\"\n",
    "\n",
    "    # grab only rows where both exist\n",
    "    d = df[[insitu_col, sat_col]].dropna()\n",
    "    if d.empty:\n",
    "        continue\n",
    "\n",
    "    # — Time series plot —\n",
    "    plt.figure(figsize=(8, 3.5))\n",
    "    plt.plot(d.index, d[insitu_col], \"-o\", label=\"In-situ\",    linewidth=1)\n",
    "    plt.plot(d.index, d[sat_col],    \"-s\", label=\"Satellite\", linewidth=1)\n",
    "    plt.title(f\"Rrs @ {wl} nm\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Rrs (sr⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # — 1:1 scatter plot —\n",
    "    mn = min(d[insitu_col].min(), d[sat_col].min())\n",
    "    mx = max(d[insitu_col].max(), d[sat_col].max())\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(d[insitu_col], d[sat_col], s=30, alpha=0.7)\n",
    "    plt.plot([mn, mx], [mn, mx], \"k--\", lw=1)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(f\"In-situ vs. Sat Rrs @ {wl} nm\")\n",
    "    plt.xlabel(\"In-situ Rrs\")\n",
    "    plt.ylabel(\"Satellite Rrs\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307f757-eee1-48a6-8186-caa3b6092ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466db4ed-705d-496e-a319-86c28351bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) rcParams for small fonts, grid, etc.\n",
    "plt.rcParams.update({\n",
    "    \"font.size\":        8,\n",
    "    \"axes.titlesize\":   9,\n",
    "    \"axes.labelsize\":   8,\n",
    "    \"xtick.labelsize\":  6,\n",
    "    \"ytick.labelsize\":  6,\n",
    "    \"legend.fontsize\":  6,\n",
    "    \"lines.linewidth\":  1.0,\n",
    "    \"lines.markersize\": 4,\n",
    "    \"figure.dpi\":       300,\n",
    "    \"figure.figsize\":  (6, 3),\n",
    "    \"axes.grid\":        True,\n",
    "    \"grid.linestyle\":   \"--\",\n",
    "    \"grid.linewidth\":   0.4,\n",
    "    \"grid.alpha\":       0.7,\n",
    "})\n",
    "\n",
    "# 2) Choose a valid style from plt.style.available\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "\n",
    "# 3) Your plotting loop\n",
    "for wl in common_wls:\n",
    "    insitu_col = str(wl)\n",
    "    sat_col    = f\"sat_rrs{wl}\"\n",
    "    d = df[[insitu_col, sat_col]].dropna()\n",
    "    if d.empty:\n",
    "        continue\n",
    "\n",
    "    # — Time series plot —\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(d.index, d[insitu_col], \"-o\", label=\"In-situ\")\n",
    "    ax.plot(d.index, d[sat_col],    \"-s\", label=\"Satellite\")\n",
    "    ax.set_title(f\"Rrs @ {wl} nm\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Rrs (sr⁻¹)\")\n",
    "    ax.legend(frameon=False)\n",
    "    fig.autofmt_xdate(rotation=30, ha=\"right\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"Rrs_timeseries_{wl}nm.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # — 1:1 scatter plot —\n",
    "    mn, mx = d.min().min(), d.max().max()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(d[insitu_col], d[sat_col], alpha=0.7)\n",
    "    ax.plot([mn, mx], [mn, mx], \"--\", linewidth=0.8)\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.set_title(f\"In-situ vs Sat Rrs @ {wl} nm\")\n",
    "    ax.set_xlabel(\"In-situ Rrs\")\n",
    "    ax.set_ylabel(\"Satellite Rrs\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"Rrs_scatter_{wl}nm.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51717b0-294c-46b7-be23-ac905a83822c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65dee7c-bf88-4220-8626-009818b2b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# assume you already have:\n",
    "#   wl          = 443\n",
    "#   insitu_col  = str(wl)\n",
    "#   sat_col     = f\"sat_rrs{wl}\"\n",
    "#   d           = df[[insitu_col, sat_col]].dropna()\n",
    "\n",
    "# 1) Compute Bland–Altman stats\n",
    "paired_mean = (d[insitu_col] + d[sat_col]) / 2\n",
    "diff        = d[sat_col] - d[insitu_col]\n",
    "\n",
    "bias  = diff.mean()\n",
    "sd    = diff.std(ddof=1)\n",
    "loa_u = bias + 1.96 * sd\n",
    "loa_l = bias - 1.96 * sd\n",
    "\n",
    "# rank correlation for Bland–Altman annotation\n",
    "rank_corr = spearmanr(d[insitu_col], d[sat_col]).correlation\n",
    "\n",
    "# 2) Compute regression & error metrics for scatter\n",
    "lr = linregress(d[insitu_col], d[sat_col])\n",
    "slope, intercept, r_lin = lr.slope, lr.intercept, lr.rvalue\n",
    "rmse = np.sqrt(mean_squared_error(d[sat_col],\n",
    "                                  intercept + slope*d[insitu_col]))\n",
    "mae  = mean_absolute_error(d[sat_col],\n",
    "                           intercept + slope*d[insitu_col])\n",
    "\n",
    "# 3) Make the two-panel figure\n",
    "fig, (ax_ba, ax_sc) = plt.subplots(1, 2, figsize=(10, 4), dpi=300)\n",
    "\n",
    "# — Bland–Altman on ax_ba —\n",
    "ax_ba.scatter(paired_mean, diff, s=20, alpha=0.6)\n",
    "ax_ba.axhline(bias,  color=\"red\",    lw=1)\n",
    "ax_ba.axhline(loa_u, color=\"forestgreen\", linestyle=\"--\", lw=1)\n",
    "ax_ba.axhline(loa_l, color=\"forestgreen\", linestyle=\"--\", lw=1)\n",
    "\n",
    "ax_ba.set_title(\"Bland–Altman Plot\")\n",
    "ax_ba.set_xlabel(f\"Paired Mean (in-situ + sat)/2\")\n",
    "ax_ba.set_ylabel(f\"Bias (sat–in-situ) [{wl} nm]\")\n",
    "\n",
    "# annotation text\n",
    "ba_text = (\n",
    "    f\"Number of Points: {len(d)}\\n\"\n",
    "    f\"Mean Bias: {bias:.2e}\\n\"\n",
    "    f\"Limits of Agreement:\\n [{loa_l:.2e}, {loa_u:.2e}]\\n\"\n",
    "    f\"Rank Corr: {rank_corr:.3f}\"\n",
    ")\n",
    "ax_ba.text(\n",
    "    0.05, 0.95, ba_text,\n",
    "    transform=ax_ba.transAxes,\n",
    "    va=\"top\", fontsize=6, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7)\n",
    ")\n",
    "\n",
    "# — Scatter + regression on ax_sc —\n",
    "ax_sc.scatter(d[insitu_col], d[sat_col], s=20, alpha=0.6)\n",
    "mn = min(d[insitu_col].min(), d[sat_col].min())\n",
    "mx = max(d[insitu_col].max(), d[sat_col].max())\n",
    "\n",
    "# 1:1 identity line\n",
    "ax_sc.plot([mn, mx], [mn, mx], color=\"black\", lw=1)\n",
    "\n",
    "# best-fit line\n",
    "xx = np.linspace(mn, mx, 100)\n",
    "yy = intercept + slope * xx\n",
    "ax_sc.plot(xx, yy, color=\"red\", linestyle=\"--\", lw=1)\n",
    "\n",
    "ax_sc.set_title(\"Scatter Plot\")\n",
    "ax_sc.set_xlabel(\"In-situ Rrs\")\n",
    "ax_sc.set_ylabel(\"Satellite Rrs\")\n",
    "\n",
    "# annotation text\n",
    "sc_text = (\n",
    "    f\"Slope: {slope:.3f}\\n\"\n",
    "    f\"Intercept: {intercept:.2e}\\n\"\n",
    "    f\"Linear Corr: {r_lin:.3f}\\n\"\n",
    "    f\"RMSE: {rmse:.2e}\\n\"\n",
    "    f\"MAE: {mae:.2e}\"\n",
    ")\n",
    "ax_sc.text(\n",
    "    0.05, 0.05, sc_text,\n",
    "    transform=ax_sc.transAxes,\n",
    "    va=\"bottom\", fontsize=6, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7)\n",
    ")\n",
    "\n",
    "# final layout\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (Optionally) save to file\n",
    "fig.savefig(f\"BlandAltman_and_Scatter_{wl}nm.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee325abd-1840-4ae9-be19-b0ba9301c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the plain‐digit in-situ bands\n",
    "insitu_wls = {int(c) for c in df.columns if c.isdigit()}\n",
    "\n",
    "# all the sat_rrs### bands\n",
    "sat_wls    = {int(c.replace(\"sat_rrs\", \"\")) \n",
    "              for c in df.columns if c.startswith(\"sat_rrs\")}\n",
    "\n",
    "# their intersection\n",
    "common_wls = sorted(insitu_wls & sat_wls)\n",
    "print(\"Can plot these wavelengths:\", common_wls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d07a98-3181-42e8-a2f9-21e74ea87a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wls = [ 560, 615, 719]   # or: plot_wls = common_wls   to do them all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627fa65-b817-413e-924f-acd4f3536683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for wl in plot_wls:\n",
    "    insitu_col = str(wl)\n",
    "    sat_col    = f\"sat_rrs{wl}\"\n",
    "    d = df[[insitu_col, sat_col]].dropna()\n",
    "    if d.empty:\n",
    "        continue\n",
    "\n",
    "    # — Bland–Altman & scatter stats like before —\n",
    "    paired_mean = 0.5*(d[insitu_col] + d[sat_col])\n",
    "    diff        = d[sat_col] - d[insitu_col]\n",
    "    bias        = diff.mean()\n",
    "    sd          = diff.std(ddof=1)\n",
    "    loa_u       = bias + 1.96*sd\n",
    "    loa_l       = bias - 1.96*sd\n",
    "    rank_corr   = spearmanr(d[insitu_col], d[sat_col]).correlation\n",
    "\n",
    "    lr          = linregress(d[insitu_col], d[sat_col])\n",
    "    slope, inter, r_lin = lr.slope, lr.intercept, lr.rvalue\n",
    "    rmse        = np.sqrt(mean_squared_error(d[sat_col],\n",
    "                                             inter + slope*d[insitu_col]))\n",
    "    mae         = mean_absolute_error(d[sat_col],\n",
    "                                      inter + slope*d[insitu_col])\n",
    "\n",
    "    # — Plotting (copy your two-panel code, just replace the hard-coded wl) —\n",
    "    fig, (ax_ba, ax_sc) = plt.subplots(1, 2, figsize=(10, 4), dpi=300)\n",
    "\n",
    "    # Bland–Altman\n",
    "    ax_ba.scatter(paired_mean, diff, s=20, alpha=0.6)\n",
    "    ax_ba.axhline(bias,      color=\"red\",    lw=1)\n",
    "    ax_ba.axhline(loa_u,     color=\"forestgreen\", ls=\"--\", lw=1)\n",
    "    ax_ba.axhline(loa_l,     color=\"forestgreen\", ls=\"--\", lw=1)\n",
    "    ax_ba.set_title(f\"Bland–Altman @ {wl} nm\")\n",
    "    ax_ba.set_xlabel(\"(in-situ + sat)/2\")\n",
    "    ax_ba.set_ylabel(f\"sat−in-situ [{wl} nm]\")\n",
    "    ba_txt = (\n",
    "        f\"N: {len(d)}\\n\"\n",
    "        f\"Bias: {bias:.2e}\\n\"\n",
    "        f\"LoA: [{loa_l:.2e}, {loa_u:.2e}]\\n\"\n",
    "        f\"ρ: {rank_corr:.3f}\"\n",
    "    )\n",
    "    ax_ba.text(0.03, 0.97, ba_txt, transform=ax_ba.transAxes,\n",
    "               va=\"top\", fontsize=6,\n",
    "               bbox=dict(facecolor=\"white\", alpha=0.7, pad=2))\n",
    "\n",
    "    # Scatter + regression\n",
    "    ax_sc.scatter(d[insitu_col], d[sat_col], s=20, alpha=0.6)\n",
    "    mn, mx = d.min().min(), d.max().max()\n",
    "    ax_sc.plot([mn, mx], [mn, mx], \"k-\", lw=1)\n",
    "    xx = np.linspace(mn, mx, 100)\n",
    "    ax_sc.plot(xx, inter + slope*xx, \"r--\", lw=1)\n",
    "    ax_sc.set_title(f\"Scatter @ {wl} nm\")\n",
    "    ax_sc.set_xlabel(\"in-situ Rrs\")\n",
    "    ax_sc.set_ylabel(\"satellite Rrs\")\n",
    "    sc_txt = (\n",
    "        f\"Slope: {slope:.3f}\\n\"\n",
    "        f\"Int: {inter:.2e}\\n\"\n",
    "        f\"r: {r_lin:.3f}\\n\"\n",
    "        f\"RMSE: {rmse:.2e}\\n\"\n",
    "        f\"MAE: {mae:.2e}\"\n",
    "    )\n",
    "    ax_sc.text(0.03, 0.05, sc_txt, transform=ax_sc.transAxes,\n",
    "               va=\"bottom\", fontsize=6,\n",
    "               bbox=dict(facecolor=\"white\", alpha=0.7, pad=2))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b22d1-aa92-4b25-b2d3-27d135af324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whole wavelenghts\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# ─── 1) Load & preprocess ─────────────────────────────────────────────────────\n",
    "# Make sure 'matchups_all.csv' is in your working directory.\n",
    "df = pd.read_csv(\"matchups_all.csv\", dtype={\"date\": str})\n",
    "\n",
    "# Parse the date (YYYYMMDD) and combine with time into a datetime index.\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\")\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"date\"].dt.strftime(\"%Y-%m-%d\") + \" \" + df[\"time\"])\n",
    "df.set_index(\"datetime\", inplace=True)\n",
    "\n",
    "# ─── 2) Discover common wavelengths ─────────────────────────────────────────────\n",
    "# In-situ bands are plain digits; satellite bands begin with 'sat_rrs'\n",
    "insitu_wls = {int(c) for c in df.columns if c.isdigit()}\n",
    "sat_wls    = {int(c.replace(\"sat_rrs\", \"\")) for c in df.columns if c.startswith(\"sat_rrs\")}\n",
    "common_wls = sorted(insitu_wls & sat_wls)\n",
    "\n",
    "print(\"Common wavelengths:\", common_wls)\n",
    "\n",
    "# ─── 3) Choose which wavelengths to plot ────────────────────────────────────────\n",
    "# To plot them all, uncomment the next line:\n",
    "plot_wls = common_wls\n",
    "\n",
    "# Or pick a subset, e.g.:\n",
    "# plot_wls = [353, 442, 447, 560]\n",
    "\n",
    "# ─── 4) Set up the publication‐style defaults ───────────────────────────────────\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\":        300,\n",
    "    \"figure.figsize\":    (10, 4),\n",
    "    \"font.size\":         8,\n",
    "    \"axes.titlesize\":    10,\n",
    "    \"axes.labelsize\":    9,\n",
    "    \"xtick.labelsize\":   7,\n",
    "    \"ytick.labelsize\":   7,\n",
    "    \"legend.fontsize\":   7,\n",
    "    \"lines.linewidth\":   1.0,\n",
    "    \"lines.markersize\":  4,\n",
    "    \"axes.grid\":         True,\n",
    "    \"grid.linestyle\":    \"--\",\n",
    "    \"grid.linewidth\":    0.5,\n",
    "    \"grid.color\":        \"0.7\",\n",
    "})\n",
    "\n",
    "# ─── 5) Loop through wavelengths and plot ───────────────────────────────────────\n",
    "for wl in plot_wls:\n",
    "    insitu_col = str(wl)\n",
    "    sat_col    = f\"sat_rrs{wl}\"\n",
    "    \n",
    "    # Subset and drop any missing pairs\n",
    "    d = df[[insitu_col, sat_col]].dropna()\n",
    "    if d.empty:\n",
    "        continue\n",
    "\n",
    "    # Bland–Altman statistics\n",
    "    paired_mean = 0.5 * (d[insitu_col] + d[sat_col])\n",
    "    diff        = d[sat_col] - d[insitu_col]\n",
    "    bias        = diff.mean()\n",
    "    sd          = diff.std(ddof=1)\n",
    "    loa_u       = bias + 1.96 * sd\n",
    "    loa_l       = bias - 1.96 * sd\n",
    "    # regression of diff vs. mean for BA trend line\n",
    "    m_ba, b_ba, *_ = linregress(paired_mean, diff)\n",
    "    # rank correlation\n",
    "    rank_corr = spearmanr(d[insitu_col], d[sat_col]).correlation\n",
    "\n",
    "    # Scatter/regression statistics\n",
    "    lr     = linregress(d[insitu_col], d[sat_col])\n",
    "    slope  = lr.slope\n",
    "    inter  = lr.intercept\n",
    "    r_lin  = lr.rvalue\n",
    "    rmse   = np.sqrt(mean_squared_error(d[sat_col],\n",
    "                                        inter + slope * d[insitu_col]))\n",
    "    mae    = mean_absolute_error(d[sat_col],\n",
    "                                 inter + slope * d[insitu_col])\n",
    "\n",
    "    # Create figure with two panels\n",
    "    fig, (ax_ba, ax_sc) = plt.subplots(1, 2)\n",
    "\n",
    "    # — Bland–Altman panel —\n",
    "    ax_ba.scatter(paired_mean, diff, s=20, alpha=0.6)\n",
    "    ax_ba.axhline(bias,      color=\"black\", lw=1)\n",
    "    ax_ba.axhline(loa_u,     color=\"black\", ls=\"--\", lw=1)\n",
    "    ax_ba.axhline(loa_l,     color=\"black\", ls=\"--\", lw=1)\n",
    "    ax_ba.plot(paired_mean, m_ba*paired_mean + b_ba,\n",
    "               ls=\"--\", color=\"crimson\", lw=1)\n",
    "\n",
    "    ax_ba.set_title(f\"Bland–Altman @ {wl} nm\")\n",
    "    ax_ba.set_xlabel(\"Paired Mean (in-situ + sat)/2\")\n",
    "    ax_ba.set_ylabel(f\"sat–in-situ [{wl} nm]\")\n",
    "\n",
    "    ba_txt = (\n",
    "        f\"N: {len(d)}\\n\"\n",
    "        f\"Bias: {bias:.2e}\\n\"\n",
    "        f\"LoA: [{loa_l:.2e}, {loa_u:.2e}]\\n\"\n",
    "        f\"ρ: {rank_corr:.3f}\"\n",
    "    )\n",
    "    ax_ba.text(0.03, 0.97, ba_txt,\n",
    "               transform=ax_ba.transAxes,\n",
    "               va=\"top\", fontsize=7,\n",
    "               bbox=dict(facecolor=\"white\", alpha=0.8, pad=2))\n",
    "\n",
    "    # — Scatter + regression panel —\n",
    "    ax_sc.scatter(d[insitu_col], d[sat_col], s=20, alpha=0.6)\n",
    "    mn, mx = d.min().min(), d.max().max()\n",
    "    ax_sc.plot([mn, mx], [mn, mx], color=\"black\", lw=1)\n",
    "    xx = np.linspace(mn, mx, 200)\n",
    "    ax_sc.plot(xx, inter + slope*xx,\n",
    "               ls=\"--\", color=\"crimson\", lw=1)\n",
    "\n",
    "    ax_sc.set_title(f\"Scatter @ {wl} nm\")\n",
    "    ax_sc.set_xlabel(\"in-situ Rrs\")\n",
    "    ax_sc.set_ylabel(\"satellite Rrs\")\n",
    "\n",
    "    sc_txt = (\n",
    "        f\"Slope: {slope:.3f}\\n\"\n",
    "        f\"Int: {inter:.2e}\\n\"\n",
    "        f\"r: {r_lin:.3f}\\n\"\n",
    "        f\"RMSE: {rmse:.2e}\\n\"\n",
    "        f\"MAE: {mae:.2e}\"\n",
    "    )\n",
    "    ax_sc.text(0.03, 0.05, sc_txt,\n",
    "               transform=ax_sc.transAxes,\n",
    "               va=\"bottom\", fontsize=7,\n",
    "               bbox=dict(facecolor=\"white\", alpha=0.8, pad=2))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # Optionally save each figure:\n",
    "    fig.savefig(f\"BlandAltman_Scatter_{wl}nm.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6c00a-2777-4980-a249-ef65a8a0c7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58513268-e773-44f2-ae12-68e2a21948cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9aae6-9731-4e5a-b701-b111c7756663",
   "metadata": {},
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10eec5-f51f-4f83-9516-a67dbef90869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import earthaccess\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f13a03-e139-47b3-980a-55746e697355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bland-Altman/Scatterplot Constants\n",
    "# Plot colors, font sizes\n",
    "COLOR_PALETTE = sns.color_palette(\"colorblind\")\n",
    "COLOR_SCATTER = COLOR_PALETTE[0]\n",
    "COLOR_LINE = \"black\"  # Was \"black\"\n",
    "COLOR_LOA = COLOR_PALETTE[2]  # Was \"green\"\n",
    "COLOR_FITLINE = COLOR_PALETTE[1]  # Was \"magenta\"\n",
    "SIZE_TITLE = 24\n",
    "SIZE_AXLABEL = 20\n",
    "SIZE_TEXTLABEL = 14\n",
    "SHOW_LEGEND = False\n",
    "\n",
    "# Update some defaults\n",
    "plt.rcParams.update({\"figure.dpi\": 300})\n",
    "sns.set_style(\"ticks\", rc={\"figure.dpi\": 300})\n",
    "sns.set_context(\"notebook\", font_scale=1.45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a4c67-5d02-41f2-a60e-a961622cc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------------------##\n",
    "#                              Plotting Utilities                             #\n",
    "##---------------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "def compute_bland_altman_metrics(xx, yy, xx_unc_modl, yy_unc_modl):\n",
    "    \"\"\"Compute metrics for Bland-Altman plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xx : array\n",
    "        Array of X data values.\n",
    "    yy : array\n",
    "        Array of Y data values.\n",
    "    xx_unc_modl : float\n",
    "        Uncertainty in X.\n",
    "    yy_unc_modl : float\n",
    "        Uncertainty in Y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of Bland-Altman metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    jj = (xx + yy) / 2\n",
    "    kk = (yy - xx) / np.sqrt((xx_unc_modl**2) + (yy_unc_modl**2))\n",
    "\n",
    "    meanbias = np.mean(kk)\n",
    "    stdbias = np.std(kk)\n",
    "    LOAlow = meanbias - stdbias\n",
    "    LOAhgh = meanbias + stdbias\n",
    "\n",
    "    ba_stat, ba_p = stats.spearmanr(jj, kk)\n",
    "    ba_independ = ba_p > 0.05\n",
    "\n",
    "    return {\n",
    "        \"count\": kk.shape[0],\n",
    "        \"jj\": jj,\n",
    "        \"kk\": kk,\n",
    "        \"meanbias\": meanbias,\n",
    "        \"LOAlow\": LOAlow,\n",
    "        \"LOAhgh\": LOAhgh,\n",
    "        \"ba_stat\": ba_stat,\n",
    "        \"ba_p\": ba_p,\n",
    "        \"ba_independ\": ba_independ,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def compute_regression_metrics(xx, yy, is_type2=False):\n",
    "    \"\"\"\n",
    "    Compute regression metrics using ordinary least-squares.\n",
    "    (We ignore the `is_type2` argument here, so you can still pass\n",
    "    is_type2=True without error.)\n",
    "    \"\"\"\n",
    "    # 1) OLS fit\n",
    "    slope, intercept, r_value, p_value, std_err = sps.linregress(xx, yy)\n",
    "\n",
    "    # 2) rank (Spearman) correlation\n",
    "    spear = sps.spearmanr(xx, yy).correlation\n",
    "\n",
    "    # 3) error metrics\n",
    "    rmse = np.sqrt(np.mean((yy - xx) ** 2))\n",
    "    mae  = np.mean(np.abs(yy - xx))\n",
    "\n",
    "    return {\n",
    "        \"count\":     len(xx),\n",
    "        \"slope\":     slope,\n",
    "        \"intercept\": intercept,\n",
    "        \"r_pear\":    r_value,\n",
    "        \"r_spear\":   spear,\n",
    "        \"rmse\":      rmse,\n",
    "        \"mae\":       mae,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def add_text_annotations(ax, text_lines, position=\"top right\", fontsize=SIZE_TEXTLABEL):\n",
    "    \"\"\"Add text annotations to the plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : Axes\n",
    "        The axis to add text to.\n",
    "    text_lines : list of str\n",
    "        List of strings to be displayed as text.\n",
    "    position : str, default 'top right'\n",
    "        Position of the text on the plot.\n",
    "    fontsize : int, default 12\n",
    "        Font size of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    if position == \"top right\":\n",
    "        x = 0.95\n",
    "        y = 0.95\n",
    "        ha = \"right\"\n",
    "        va = \"top\"\n",
    "    elif position == \"top left\":\n",
    "        x = 0.05\n",
    "        y = 0.95\n",
    "        ha = \"left\"\n",
    "        va = \"top\"\n",
    "    elif position == \"bottom left\":\n",
    "        x = 0.05\n",
    "        y = 0.05\n",
    "        ha = \"left\"\n",
    "        va = \"bottom\"\n",
    "    elif position == \"bottom right\":\n",
    "        x = 0.95\n",
    "        y = 0.05\n",
    "        ha = \"right\"\n",
    "        va = \"bottom\"\n",
    "\n",
    "    text = \"\\n\".join(text_lines)\n",
    "    ax.text(\n",
    "        x,\n",
    "        y,\n",
    "        text,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=fontsize,\n",
    "        verticalalignment=va,\n",
    "        horizontalalignment=ha,\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.6, edgecolor=\"none\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def setup_plot(label):\n",
    "    \"\"\"Set up the plot with titles and labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : str\n",
    "        Title of the plot.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Figure and axes of the plot.\n",
    "\n",
    "    \"\"\"\n",
    "    style.use(\"seaborn-v0_8-whitegrid\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), layout=\"constrained\")\n",
    "    fig.suptitle(label, fontsize=22)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "\n",
    "def format_ticks(ax):\n",
    "    \"\"\"Format the tick labels on the axes to be more readable.\"\"\"\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:.3g}\"))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f\"{y:.3g}\"))\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", width=2, length=6)\n",
    "    ax.spines[\"top\"].set_linewidth(2)\n",
    "    ax.spines[\"right\"].set_linewidth(2)\n",
    "    ax.spines[\"left\"].set_linewidth(2)\n",
    "    ax.spines[\"bottom\"].set_linewidth(2)\n",
    "\n",
    "\n",
    "def plot_bland_altman(\n",
    "    ax1,\n",
    "    metrics,\n",
    "    binscale,\n",
    "    xx_unc_modl,\n",
    "    x_label=\"x\",\n",
    "    y_label=\"y\"\n",
    "):\n",
    "    \"\"\"Plot Bland-Altman plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax1 : Axes\n",
    "        Axis for the Bland-Altman plot.\n",
    "    metrics : dict\n",
    "        Bland-Altman metrics.\n",
    "    binscale : float\n",
    "        Scaling factor for bin size.\n",
    "    xx_unc_modl : float\n",
    "        Uncertainty in X.\n",
    "    x_label : string, default \"x\"\n",
    "        String for labels for x data\n",
    "    y_label : string, default \"y\"\n",
    "        String for labels for y data\n",
    "\n",
    "    \"\"\"\n",
    "    jj = metrics[\"jj\"]\n",
    "    kk = metrics[\"kk\"]\n",
    "    npoints = metrics[\"count\"]\n",
    "    meanbias = metrics[\"meanbias\"]\n",
    "    LOAlow = metrics[\"LOAlow\"]\n",
    "    LOAhgh = metrics[\"LOAhgh\"]\n",
    "    ba_independ = metrics[\"ba_independ\"]\n",
    "    ba_stat = metrics[\"ba_stat\"]\n",
    "\n",
    "    min_kk = meanbias - 5 * np.std(kk)\n",
    "    max_kk = meanbias + 5 * np.std(kk)\n",
    "    min_jj = np.min(jj)\n",
    "    max_jj = np.max(jj)\n",
    "    lineclr, loaclr, fitclr = (COLOR_LINE, COLOR_LOA, COLOR_FITLINE)\n",
    "    ax1.scatter(jj, kk, color=COLOR_SCATTER)\n",
    "    ax1.set_xlim([min_jj, max_jj])\n",
    "    ax1.set_ylim([min_kk, max_kk])\n",
    "\n",
    "    ax1.set_title(\"Bland-Altman plot\", fontsize=SIZE_TITLE)\n",
    "    ylabel = (\n",
    "        \"Uncertainty normalized bias\"\n",
    "        if xx_unc_modl != np.sqrt(0.5)\n",
    "        else f\"Bias, ${y_label}-{x_label}$\"\n",
    "    )\n",
    "    ax1.set_ylabel(ylabel, fontsize=SIZE_AXLABEL)\n",
    "    ax1.set_xlabel(\n",
    "        f\"Paired mean, $({x_label}+{y_label})/2$\", fontsize=SIZE_AXLABEL\n",
    "        )\n",
    "    ax1.plot(\n",
    "        [min_jj, max_jj], [0, 0],\n",
    "        color=lineclr, linestyle=\"solid\", linewidth=4.0\n",
    "        )\n",
    "\n",
    "    if ba_independ:\n",
    "        ax1.plot(\n",
    "            [min_jj, max_jj],\n",
    "            [meanbias, meanbias],\n",
    "            color=fitclr,\n",
    "            linestyle=\"dashed\",\n",
    "            linewidth=3.0,\n",
    "            label=\"Mean Bias\",\n",
    "        )\n",
    "        ax1.plot(\n",
    "            [min_jj, max_jj],\n",
    "            [LOAlow, LOAlow],\n",
    "            color=loaclr,\n",
    "            linestyle=\"dashed\",\n",
    "            linewidth=2.0,\n",
    "            label=\"Lower LOA\",\n",
    "        )\n",
    "        ax1.plot(\n",
    "            [min_jj, max_jj],\n",
    "            [LOAhgh, LOAhgh],\n",
    "            color=loaclr,\n",
    "            linestyle=\"dashed\",\n",
    "            linewidth=2.0,\n",
    "            label=\"Upper LOA\",\n",
    "        )\n",
    "        ax1.fill_between(\n",
    "            [min_jj, max_jj], LOAlow, LOAhgh,\n",
    "            color=loaclr, alpha=0.1\n",
    "            )\n",
    "    else:\n",
    "        ba_regress_result = stats.linregress(jj, kk)\n",
    "        ba_min_fit_yy = ba_regress_result.slope * min_jj + ba_regress_result.intercept\n",
    "        ba_max_fit_yy = ba_regress_result.slope * max_jj + ba_regress_result.intercept\n",
    "        ax1.plot(\n",
    "            [min_jj, max_jj],\n",
    "            [ba_min_fit_yy, ba_max_fit_yy],\n",
    "            color=fitclr,\n",
    "            linestyle=\"dashed\",\n",
    "            linewidth=3.0,\n",
    "            label=\"Linear Fit\",\n",
    "        )\n",
    "    if SHOW_LEGEND:\n",
    "        ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    format_ticks(ax1)\n",
    "\n",
    "    text_lines = [\n",
    "        f\"Number of Points: {npoints}\",\n",
    "        f\"Mean Bias: {meanbias:.2e}\",\n",
    "        f\"Limits of Agreement: [{LOAlow:.2e}, {LOAhgh:.2e}]\",\n",
    "        f\"Rank Correlation: {ba_stat:.3f}\",\n",
    "        \"Bias Independent\" if ba_independ else \"Bias Dependent\",\n",
    "    ]\n",
    "    add_text_annotations(ax1, text_lines, position=\"bottom right\")\n",
    "\n",
    "\n",
    "def plot_scatter(\n",
    "    ax2, xx, yy, regress_metrics, binscale, x_label=\"x\", y_label=\"y\"\n",
    "):\n",
    "    \"\"\"Plot scatter plot with regression line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax2 : Axes\n",
    "        Axis for the scatter plot.\n",
    "    xx : array\n",
    "        Array of X data values.\n",
    "    yy : array\n",
    "        Array of Y data values.\n",
    "    regress_metrics : dict\n",
    "        Regression metrics.\n",
    "    binscale : float\n",
    "        Scaling factor for bin size.\n",
    "    x_label : string, default \"x\"\n",
    "        String for labels for x data\n",
    "    y_label : string, default \"y\"\n",
    "        String for labels for y data\n",
    "\n",
    "    \"\"\"\n",
    "    min_val = min(np.min(xx), np.min(yy))\n",
    "    max_val = max(np.max(xx), np.max(yy))\n",
    "\n",
    "    ax2.scatter(xx, yy, color=COLOR_SCATTER)\n",
    "    ax2.set_xlim([min_val, max_val])\n",
    "    ax2.set_ylim([min_val, max_val])\n",
    "\n",
    "    ax2.set_title(\"Scatterplot\", fontsize=SIZE_TITLE)\n",
    "    ax2.set_xlabel(f\"${x_label}$\", fontsize=SIZE_AXLABEL)\n",
    "    ax2.set_ylabel(f\"${y_label}$\", fontsize=SIZE_AXLABEL)\n",
    "    ax2.plot(\n",
    "        [min_val, max_val],\n",
    "        [min_val, max_val],\n",
    "        color=COLOR_LINE,\n",
    "        linestyle=\"solid\",\n",
    "        linewidth=4.0,\n",
    "    )\n",
    "\n",
    "    slope = regress_metrics[\"slope\"]\n",
    "    intercept = regress_metrics[\"intercept\"]\n",
    "    min_fit_yy = slope * min_val + intercept\n",
    "    max_fit_yy = slope * max_val + intercept\n",
    "    ax2.plot(\n",
    "        [min_val, max_val],\n",
    "        [min_fit_yy, max_fit_yy],\n",
    "        color=COLOR_FITLINE,\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=3.0,\n",
    "        label=\"Regression Line\",\n",
    "    )\n",
    "    if SHOW_LEGEND:\n",
    "        ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    format_ticks(ax2)\n",
    "\n",
    "    text_lines = [\n",
    "        f\"Slope: {slope:.3f}\",\n",
    "        f\"Intercept: {intercept:.2e}\",\n",
    "        f\"Linear Correlation: {regress_metrics['r_pear']:.3f}\",\n",
    "        f\"Rank Correlation: {regress_metrics['r_spear']:.3f}\",\n",
    "        f\"RMSE: {regress_metrics['rmse']:.2e}\",\n",
    "        f\"MAE: {regress_metrics['mae']:.2e}\",\n",
    "    ]\n",
    "    add_text_annotations(ax2, text_lines, position=\"bottom right\")\n",
    "\n",
    "\n",
    "def plot_BAvsScat(\n",
    "    x_input,\n",
    "    y_input,\n",
    "    label=\"\",\n",
    "    saveplot=None,\n",
    "    binscale=1.0,\n",
    "    xx_unc_modl=np.sqrt(0.5),\n",
    "    yy_unc_modl=np.sqrt(0.5),\n",
    "    x_label=\"x\",\n",
    "    y_label=\"y\",\n",
    "    is_type2=True,\n",
    "):\n",
    "    \"\"\"Routine to plot paired data as Bland-Altman and scatter plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_input : array-like\n",
    "        Array of X data values.\n",
    "    y_input : array-like\n",
    "        Corresponding array of Y data values.\n",
    "    label : string, default ''\n",
    "        Text label for plotting.\n",
    "    saveplot : string, default None\n",
    "        Set to save plot in ../output/ with the string as the filename.\n",
    "    binscale : float, default 1.0\n",
    "        Scaling factor for how many bins to include in a 2D histogram.\n",
    "    xx_unc_modl : float, default np.sqrt(0.5)\n",
    "        Uncertainty in X.\n",
    "    yy_unc_modl : float, default np.sqrt(0.5)\n",
    "        Uncertainty in Y.\n",
    "    x_label : string, default \"x\"\n",
    "        String for labels for x data\n",
    "    y_label : string, default \"y\"\n",
    "        String for labels for y data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of computed statistics.\n",
    "\n",
    "    \"\"\"\n",
    "    xx = np.asarray(x_input)\n",
    "    yy = np.asarray(y_input)\n",
    "    valid_indices = (\n",
    "        np.isfinite(x_input)\n",
    "        & np.isfinite(y_input)\n",
    "        & (x_input != -999)\n",
    "        & (y_input != -999)\n",
    "    )\n",
    "    xx = x_input[valid_indices]\n",
    "    yy = y_input[valid_indices]\n",
    "\n",
    "    ba_metrics = compute_bland_altman_metrics(xx, yy, xx_unc_modl, yy_unc_modl)\n",
    "    regress_metrics = compute_regression_metrics(xx, yy, is_type2=is_type2)\n",
    "\n",
    "    fig, ax1, ax2 = setup_plot(label)\n",
    "    plot_bland_altman(ax1, ba_metrics, binscale, xx_unc_modl, x_label, y_label)\n",
    "    plot_scatter(ax2, xx, yy, regress_metrics, binscale, x_label, y_label)\n",
    "\n",
    "    if saveplot is not None:\n",
    "        figpath = Path(\"../output\") / saveplot\n",
    "        fig.savefig(figpath)\n",
    "        print(\"Saved figure to:\", figpath)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"Number_of_Points\": ba_metrics[\"count\"],\n",
    "        \"Scale_Independence\": ba_metrics[\"ba_independ\"],\n",
    "        \"Mean_Bias\": ba_metrics[\"meanbias\"],\n",
    "        \"Limits_of_Agreement_low\": (\n",
    "            ba_metrics[\"LOAlow\"] if ba_metrics[\"ba_independ\"] else float(\"nan\")\n",
    "        ),\n",
    "        \"Limits_of_Agreement_high\": (\n",
    "            ba_metrics[\"LOAhgh\"] if ba_metrics[\"ba_independ\"] else float(\"nan\")\n",
    "        ),\n",
    "        \"Linear_Slope\": regress_metrics[\"slope\"],\n",
    "        \"Linear_Intercept\": regress_metrics[\"intercept\"],\n",
    "        \"Linear_Correlation\": regress_metrics[\"r_pear\"],\n",
    "        \"Rank_Correlation\": regress_metrics[\"r_spear\"],\n",
    "        \"RMSE\": regress_metrics[\"rmse\"],\n",
    "        \"MAE\": regress_metrics[\"mae\"],\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
